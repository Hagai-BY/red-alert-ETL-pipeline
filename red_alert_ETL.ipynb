{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRed Alert ETL Pipeline\\n\\nThis script implements an ETL (Extract, Transform, Load) pipeline for processing\\nIsraeli Red Alert (air raid siren) data. It fetches data from the Oref API,\\nadds geographic region information, and filters by specific regions and dates.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Red Alert ETL Pipeline\n",
    "\n",
    "This script implements an ETL (Extract, Transform, Load) pipeline for processing\n",
    "Israeli Red Alert (air raid siren) data. It fetches data from the Oref API,\n",
    "adds geographic region information, and filters by specific regions and dates.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_high_volume_days(start_date, end_date, hours_per_chunk=6):\n",
    "    \"\"\"\n",
    "    Special function to fetch data for high-volume days like October 7-8, 2023\n",
    "    using very small time chunks to avoid API limitations.\n",
    "    \n",
    "    Args:\n",
    "        start_date (datetime): Start date\n",
    "        end_date (datetime): End date  \n",
    "        hours_per_chunk (int): Hours per chunk (default: 6 hours)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with all alerts from the high-volume period\n",
    "    \"\"\"\n",
    "    all_alerts = []\n",
    "    all_unique_locations = set()\n",
    "    \n",
    "    # Create hourly chunks for these critical days\n",
    "    current_time = start_date\n",
    "    chunk_times = []\n",
    "    \n",
    "    while current_time < end_date:\n",
    "        chunk_end = current_time + timedelta(hours=hours_per_chunk)\n",
    "        if chunk_end > end_date:\n",
    "            chunk_end = end_date\n",
    "        chunk_times.append((current_time, chunk_end))\n",
    "        current_time = chunk_end\n",
    "    \n",
    "    print(f\"\\n=== Special Processing for High Volume Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')} ===\")\n",
    "    print(f\"Breaking into {len(chunk_times)} smaller chunks of {hours_per_chunk} hours each\")\n",
    "    \n",
    "    for chunk_idx, (chunk_start, chunk_end) in enumerate(chunk_times):\n",
    "        # Add small delay between requests\n",
    "        if chunk_idx > 0:\n",
    "            delay = 3  # Slightly longer delay for high-volume requests\n",
    "            print(f\"Waiting {delay} seconds before next high-volume request...\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        # Format with both date and time for logging\n",
    "        start_str = chunk_start.strftime('%Y-%m-%d %H:%M')\n",
    "        end_str = chunk_end.strftime('%Y-%m-%d %H:%M')\n",
    "        print(f\"Fetching chunk {chunk_idx+1}/{len(chunk_times)}: {start_str} to {end_str}...\")\n",
    "        \n",
    "        # Use the existing fetch function but with date only (API limitation)\n",
    "        # We'll filter by time later if needed\n",
    "        chunk_df = fetch_alerts(chunk_start.replace(hour=0, minute=0), \n",
    "                               chunk_end.replace(hour=23, minute=59))\n",
    "        \n",
    "        if not chunk_df.empty:\n",
    "            # Filter to the exact time window since API only accepts date granularity\n",
    "            if 'date' in chunk_df.columns and 'time' in chunk_df.columns:\n",
    "                print(f\"  Before filtering: {len(chunk_df)} records\")\n",
    "                \n",
    "                # Convert string times to datetime for proper filtering\n",
    "                try:\n",
    "                    # Create a datetime column for filtering\n",
    "                    chunk_df['datetime'] = pd.to_datetime(\n",
    "                        chunk_df['date'] + ' ' + chunk_df['time'], \n",
    "                        format='%d.%m.%Y %H:%M:%S',\n",
    "                        errors='coerce'  # This keeps invalid dates as NaT instead of raising errors\n",
    "                    )\n",
    "                    \n",
    "                    # Display sample datetimes for debugging\n",
    "                    print(f\"  Sample datetimes: {chunk_df['datetime'].head(3).tolist()}\")\n",
    "                    print(f\"  Filter range: {chunk_start} to {chunk_end}\")\n",
    "                    \n",
    "                    # Filter by time range\n",
    "                    filtered_df = chunk_df[(chunk_df['datetime'] >= chunk_start) & \n",
    "                                          (chunk_df['datetime'] < chunk_end)]\n",
    "                    \n",
    "                    print(f\"  After filtering: {len(filtered_df)} records\")\n",
    "                    \n",
    "                    # If filtering removed all records, check for date format issues\n",
    "                    if filtered_df.empty and not chunk_df.empty:\n",
    "                        print(\"  Warning: Time filtering removed all records, checking date formats...\")\n",
    "                        # Try alternative approach - use hour extraction\n",
    "                        chunk_hour = pd.to_datetime(chunk_df['time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "                        start_hour = chunk_start.hour\n",
    "                        end_hour = chunk_end.hour\n",
    "                        \n",
    "                        # Filter by hour range\n",
    "                        filtered_df = chunk_df[(chunk_hour >= start_hour) & (chunk_hour < end_hour)]\n",
    "                        print(f\"  After hour-based filtering: {len(filtered_df)} records\")\n",
    "                        \n",
    "                        # If still empty, keep all records for this chunk as a fallback\n",
    "                        if filtered_df.empty:\n",
    "                            print(\"  Warning: Hour filtering also failed. Keeping all records for this day.\")\n",
    "                            filtered_df = chunk_df.copy()\n",
    "                    \n",
    "                    # Drop the temporary datetime column\n",
    "                    if 'datetime' in filtered_df.columns:\n",
    "                        filtered_df = filtered_df.drop('datetime', axis=1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error during time filtering: {str(e)}\")\n",
    "                    print(\"  Keeping all records for this day instead.\")\n",
    "                    filtered_df = chunk_df.copy()\n",
    "                \n",
    "                # Only continue if we have data after filtering\n",
    "                if not filtered_df.empty:\n",
    "                    # Debug: Log unique locations in this chunk\n",
    "                    chunk_locations = set()\n",
    "                    for loc_str in filtered_df['data'].dropna():\n",
    "                        locations = [loc.strip() for loc in loc_str.split(',')]\n",
    "                        chunk_locations.update(locations)\n",
    "                    all_unique_locations.update(chunk_locations)\n",
    "                    \n",
    "                    print(f\"  Found {len(filtered_df)} alerts with {len(chunk_locations)} unique locations in this time chunk.\")\n",
    "                    all_alerts.append(filtered_df)\n",
    "                else:\n",
    "                    print(f\"  No alerts found in this specific time window after filtering.\")\n",
    "            else:\n",
    "                # If we can't filter by time, just use the whole chunk\n",
    "                chunk_locations = set()\n",
    "                for loc_str in chunk_df['data'].dropna():\n",
    "                    locations = [loc.strip() for loc in loc_str.split(',')]\n",
    "                    chunk_locations.update(locations)\n",
    "                all_unique_locations.update(chunk_locations)\n",
    "                \n",
    "                print(f\"  Found {len(chunk_df)} alerts with {len(chunk_locations)} unique locations in this chunk.\")\n",
    "                all_alerts.append(chunk_df)\n",
    "        else:\n",
    "            print(f\"  No data found in this time range.\")\n",
    "    \n",
    "    if all_alerts:\n",
    "        combined_df = pd.concat(all_alerts, ignore_index=True)\n",
    "        print(f\"\\nHigh volume period summary:\")\n",
    "        print(f\"Total alerts: {len(combined_df)}\")\n",
    "        print(f\"Total unique locations: {len(all_unique_locations)}\")\n",
    "        \n",
    "        # Save unique locations from this period to a separate file\n",
    "        period_name = f\"{start_date.strftime('%Y%m%d')}to{end_date.strftime('%Y%m%d')}\"\n",
    "        locations_file = f\"locations_{period_name}.txt\"\n",
    "        with open(locations_file, 'w', encoding='utf-8') as f:\n",
    "            for loc in sorted(all_unique_locations):\n",
    "                f.write(f\"{loc}\\n\")\n",
    "        print(f\"Unique locations for this period saved to {locations_file}\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No alerts data found for the high-volume period.\")\n",
    "        return pd.DataFrame(columns=['data', 'date', 'time', 'category', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# API settings\n",
    "OREF_API_URL = \"https://alerts-history.oref.org.il/Shared/Ajax/GetAlarmsHistory.aspx\"\n",
    "NOMINATIM_API_URL = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "# Date range settings\n",
    "START_DATE = datetime(2023, 10, 1)\n",
    "END_DATE = datetime(2023, 12, 31)\n",
    "\n",
    "# API request settings\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5  # seconds\n",
    "\n",
    "# Encoding settings\n",
    "CSV_ENCODING = 'utf-8-sig'  # UTF-8 with BOM for Hebrew\n",
    "EXCEL_ENGINE = 'openpyxl'   # Better support for Hebrew\n",
    "\n",
    "# Specific dates to filter (66 days)\n",
    "FILTER_DATES = [\n",
    "    '2023-10-01', '2023-10-02', '2023-10-03', '2023-10-04', '2023-10-05',\n",
    "    '2023-10-08', '2023-10-09', '2023-10-10', '2023-10-11', '2023-10-12',\n",
    "    '2023-10-15', '2023-10-16', '2023-10-17', '2023-10-18', '2023-10-19',\n",
    "    '2023-10-22', '2023-10-23', '2023-10-24', '2023-10-25', '2023-10-26',\n",
    "    '2023-10-29', '2023-10-30', '2023-10-31', '2023-11-01', '2023-11-02',\n",
    "    '2023-11-05', '2023-11-06', '2023-11-07', '2023-11-08', '2023-11-09',\n",
    "    '2023-11-12', '2023-11-13', '2023-11-14', '2023-11-15', '2023-11-16',\n",
    "    '2023-11-19', '2023-11-20', '2023-11-21', '2023-11-22', '2023-11-23',\n",
    "    '2023-11-26', '2023-11-27', '2023-11-28', '2023-11-29', '2023-11-30',\n",
    "    '2023-12-03', '2023-12-04', '2023-12-05', '2023-12-06', '2023-12-07',\n",
    "    '2023-12-10', '2023-12-11', '2023-12-12', '2023-12-13', '2023-12-14',\n",
    "    '2023-12-17', '2023-12-18', '2023-12-19', '2023-12-20', '2023-12-21',\n",
    "    '2023-12-24', '2023-12-25', '2023-12-26', '2023-12-27', '2023-12-28',\n",
    "    '2023-12-31'\n",
    "]\n",
    "\n",
    "# Region definitions\n",
    "CENTRAL_REGIONS = [\n",
    "    'CENTRAL_DISTRICT',\n",
    "    'TEL_AVIV_DISTRICT',\n",
    "    'DAN_DISTRICT',\n",
    "    'SHARON_DISTRICT',\n",
    "    'SHFELA_DISTRICT'\n",
    "]\n",
    "\n",
    "# Region translation mapping\n",
    "REGION_TRANSLATION = {\n",
    "    'מחוז המרכז': 'CENTRAL_DISTRICT',\n",
    "    'מחוז הצפון': 'NORTHERN_DISTRICT',\n",
    "    'מחוז הדרום': 'SOUTHERN_DISTRICT',\n",
    "    'מחוז ירושלים': 'JERUSALEM_DISTRICT',\n",
    "    'מחוז תל אביב': 'TEL_AVIV_DISTRICT',\n",
    "    'מחוז חיפה': 'HAIFA_DISTRICT',\n",
    "    'יהודה ושומרון': 'JUDEA_AND_SAMARIA',\n",
    "    'עוטף עזה': 'GAZA_ENVELOPE',\n",
    "    'DAN': 'DAN_DISTRICT',\n",
    "    'SHARON': 'SHARON_DISTRICT',\n",
    "    'CENTRAL': 'CENTRAL_DISTRICT',\n",
    "    'NORTH': 'NORTHERN_DISTRICT',\n",
    "    'SOUTH': 'SOUTHERN_DISTRICT',\n",
    "    'JERUSALEM': 'JERUSALEM_DISTRICT',\n",
    "    'WEST_BANK': 'JUDEA_AND_SAMARIA',\n",
    "    'SHFELA': 'SHFELA_DISTRICT',\n",
    "    'JORDAN_VALLEY': 'JORDAN_VALLEY_DISTRICT',\n",
    "    'GAZA_ENVELOPE': 'GAZA_ENVELOPE',\n",
    "    \"CENTRAL_DISTRICT\": 'CENTRAL_DISTRICT',\n",
    "    \"JUDEA_AND_SAMARIA\": 'JUDEA_AND_SAMARIA',\n",
    "    \"NORTHERN_DISTRICT\": 'NORTHERN_DISTRICT',\n",
    "    \"JERUSALEM_DISTRICT\": 'JERUSALEM_DISTRICT',\n",
    "    \"SOUTHERN_DISTRICT\": 'SOUTHERN_DISTRICT',\n",
    "    \"HAIFA_DISTRICT\": 'HAIFA_DISTRICT',\n",
    "}\n",
    "\n",
    "# File paths\n",
    "MERGED_ALARMS_PATH = 'merged_alarms_data.csv'\n",
    "CITIES_REGIONS_PATH = 'israel_cities_regions.xlsx'\n",
    "CITIES_REGIONS_UPDATED_PATH = 'israel_cities_regions_updated.xlsx'\n",
    "ALARMS_WITH_REGIONS_PATH = 'alarms_with_regions.xlsx'\n",
    "ALARMS_WITH_ENGLISH_REGIONS_PATH = 'alarms_with_english_regions.xlsx'\n",
    "CENTRAL_REGIONS_ALARMS_PATH = 'central_regions_alarms.xlsx'\n",
    "FINAL_FILTERED_ALARMS_PATH = 'central_regions_66days_sorted.xlsx'\n",
    "\n",
    "# Encoding settings\n",
    "CSV_ENCODING = 'utf-8'\n",
    "EXCEL_ENGINE = 'openpyxl'\n",
    "\n",
    "# Manual region mappings for locations that couldn't be geocoded\n",
    "# Manual region mappings for locations that couldn't be geocoded\n",
    "MANUAL_REGION_MAPPINGS = {\n",
    "    # Industrial zones in the south\n",
    "    'אזור תעשייה הדרומי אשקלון': 'SOUTH',\n",
    "    'אזור תעשייה ברוש': 'SOUTH',\n",
    "    'אזור תעשייה תימורים': 'SOUTH',\n",
    "    'אזור תעשייה עידן הנגב': 'SOUTH',\n",
    "    'אזור תעשייה עד הלום': 'SOUTH',\n",
    "    'אזור תעשייה קריית גת': 'SOUTH',\n",
    "    \"אזור תעשייה נ.ע.מ\": 'SOUTH',\n",
    "    \n",
    "    # Industrial zones in the center\n",
    "    'אזור תעשייה נשר - רמלה': 'CENTRAL',\n",
    "    'אזור תעשייה חבל מודיעין': 'CENTRAL',\n",
    "    'אזור תעשייה אפק ולב הארץ': 'CENTRAL',\n",
    "    'אזור תעשייה כנות': 'CENTRAL',\n",
    "    'פארק תעשייה ראם': 'CENTRAL',\n",
    "    \n",
    "    # Industrial zones in the north\n",
    "    'אזור תעשייה מבואות הגלבוע': 'NORTH',\n",
    "    'אזור תעשייה אלון התבור': 'NORTH',\n",
    "    'אזור תעשייה רמת דלתון': 'NORTH',\n",
    "    'אזור תעשייה קדמת גליל': 'NORTH',\n",
    "    'אזור תעשייה אכזיב מילואות': 'NORTH',\n",
    "    'אזור תעשייה צמח': 'NORTH',\n",
    "    'אזור תעשייה צ.ח.ר': 'NORTH',\n",
    "    'אזור תעשייה תרדיון': 'NORTH',\n",
    "    'עכו - אזור תעשייה': 'NORTH',\n",
    "    \"מרכז חבר\": 'HAIFA_DISTRICT',\n",
    "    \n",
    "    # City parts\n",
    "    'הרצליה - מרכז וגליל ים': 'SHARON',\n",
    "    'הרצליה - מערב': 'SHARON',\n",
    "    'באר שבע - מזרח': 'SOUTH',\n",
    "    'באר שבע - מערב': 'SOUTH',\n",
    "    'תל אביב - דרום העיר ויפו': 'DAN',\n",
    "    'רמת גן - מזרח': 'DAN',\n",
    "    'ראשון לציון - מזרח': 'CENTRAL',\n",
    "    'אשדוד - איזור תעשייה צפוני': 'SOUTH',\n",
    "    \n",
    "    # NEW MAPPINGS FOR PREVIOUSLY UNKNOWN LOCATIONS\n",
    "    \n",
    "    # Judea and Samaria\n",
    "    'אלון שבות': 'JUDEA_AND_SAMARIA',\n",
    "    'אלפי מנשה': 'JUDEA_AND_SAMARIA',\n",
    "    'ברוכין': 'JUDEA_AND_SAMARIA',\n",
    "    'גבע בנימין': 'JUDEA_AND_SAMARIA',\n",
    "    'גבעון החדשה': 'JUDEA_AND_SAMARIA',\n",
    "    'מודיעין עילית': 'JUDEA_AND_SAMARIA',\n",
    "    'מעלה אפרים': 'JUDEA_AND_SAMARIA',\n",
    "    'מעלה מכמש': 'JUDEA_AND_SAMARIA',\n",
    "    'נבי סמואל': 'JUDEA_AND_SAMARIA',\n",
    "    'נווה דניאל': 'JUDEA_AND_SAMARIA',\n",
    "    'נופי פרת': 'JUDEA_AND_SAMARIA',\n",
    "    'עלי זהב': 'JUDEA_AND_SAMARIA',\n",
    "    'ראש צורים': 'JUDEA_AND_SAMARIA',\n",
    "    'אזור תעשייה אריאל': 'JUDEA_AND_SAMARIA',\n",
    "    'אזור תעשייה ברקן': 'JUDEA_AND_SAMARIA',\n",
    "    'פארק תעשיות מגדל עוז': 'JUDEA_AND_SAMARIA',\n",
    "    \"ענב\": 'JUDEA_AND_SAMARIA',\n",
    "    \"סלעית\": 'JUDEA_AND_SAMARIA',\n",
    "\n",
    "    # Northern District\n",
    "    'בית סוהר צלמון': 'NORTHERN_DISTRICT',\n",
    "    'בני יהודה וגבעת יואב': 'NORTHERN_DISTRICT',\n",
    "    'גורנות הגליל': 'NORTHERN_DISTRICT',\n",
    "    'ח\\'וואלד': 'NORTHERN_DISTRICT',\n",
    "    'אל-ח\\'וואלד מערב': 'NORTHERN_DISTRICT',\n",
    "    'כאוכב אבו אלהיג\\'א': 'NORTHERN_DISTRICT',\n",
    "    'כורזים ורד הגליל': 'NORTHERN_DISTRICT',\n",
    "    'מנחת מחניים': 'NORTHERN_DISTRICT',\n",
    "    'מצוק עורבים': 'NORTHERN_DISTRICT',\n",
    "    'מרכז אזורי מבואות חרמון': 'NORTHERN_DISTRICT',\n",
    "    'מרכז אזורי מרום גליל': 'NORTHERN_DISTRICT',\n",
    "    'סואעד חמירה': 'NORTHERN_DISTRICT',\n",
    "    'קצרין - אזור תעשייה': 'NORTHERN_DISTRICT',\n",
    "    'צפת - נוף כנרת': 'NORTHERN_DISTRICT',\n",
    "    'צפת - עיר': 'NORTHERN_DISTRICT',\n",
    "    'צפת - עכברה': 'NORTHERN_DISTRICT',\n",
    "    'קבוצת גבע': 'NORTHERN_DISTRICT',\n",
    "    'ישובי אומן': 'NORTHERN_DISTRICT',\n",
    "    'ישובי יעל': 'NORTHERN_DISTRICT',\n",
    "    'אתר ההנצחה גולני': 'NORTHERN_DISTRICT',\n",
    "\n",
    "    # Central District\n",
    "    'אתר חירייה': 'CENTRAL_DISTRICT',\n",
    "    'כפר נוער בן שמן': 'CENTRAL_DISTRICT',\n",
    "    'מודיעין - ישפרו סנטר': 'CENTRAL_DISTRICT',\n",
    "    'מודיעין - ליגד סנטר': 'CENTRAL_DISTRICT',\n",
    "    'מרכז אזורי דרום השרון': 'CENTRAL_DISTRICT',\n",
    "    'מתחם פי גלילות': 'CENTRAL_DISTRICT',\n",
    "    'אזור תעשיה רגם': 'CENTRAL_DISTRICT',\n",
    "    \"כפר הרי''ף וצומת ראם\": 'CENTRAL_DISTRICT',\n",
    "\n",
    "    # Jerusalem District\n",
    "    'פנימיית עין כרם': 'JERUSALEM_DISTRICT',\n",
    "    'אזור תעשייה הר טוב - צרעה': 'JERUSALEM_DISTRICT',\n",
    "\n",
    "    # Southern District\n",
    "    'ואדי אל נעם דרום': 'SOUTHERN_DISTRICT',\n",
    "    'חוות שיקמים': 'SOUTHERN_DISTRICT',\n",
    "    'כפר הרי\"ף וצומת ראם': 'SOUTHERN_DISTRICT',\n",
    "    'כפר מימון ותושיה': 'SOUTHERN_DISTRICT',\n",
    "    'מטווח ניר עם': 'SOUTHERN_DISTRICT',\n",
    "    'מתחם בני דרום': 'SOUTHERN_DISTRICT',\n",
    "    'סעייה-מולדה': 'SOUTHERN_DISTRICT',\n",
    "    'קריית חינוך מרחבים': 'SOUTHERN_DISTRICT',\n",
    "    'תחנת רכבת קריית מלאכי - יואב': 'SOUTHERN_DISTRICT',\n",
    "    'תעשיון חצב': 'SOUTHERN_DISTRICT',\n",
    "    'תקומה וחוות יזרעם': 'SOUTHERN_DISTRICT',\n",
    "    'אזור תעשייה שחק': 'SOUTHERN_DISTRICT',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove quotes and extra spaces\n",
    "    text = text.replace('\"', '').strip()\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_date_range_chunks(start_date, end_date, chunk_days=14):\n",
    "    \"\"\"\n",
    "    Split date range into chunks of specified days.\n",
    "    \n",
    "    Args:\n",
    "        start_date (datetime): Start date\n",
    "        end_date (datetime): End date\n",
    "        chunk_days (int): Number of days in each chunk\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (chunk_start, chunk_end) tuples\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_start = start_date\n",
    "    \n",
    "    while current_start < end_date:\n",
    "        current_end = current_start + timedelta(days=chunk_days)\n",
    "        if current_end > end_date:\n",
    "            current_end = end_date\n",
    "        \n",
    "        chunks.append((current_start, current_end))\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def optimize_excel_columns(worksheet, dataframe):\n",
    "    \"\"\"\n",
    "    Optimize column widths in Excel worksheet based on content.\n",
    "    \n",
    "    Args:\n",
    "        worksheet: Excel worksheet object\n",
    "        dataframe (DataFrame): DataFrame with the data\n",
    "    \"\"\"\n",
    "    for idx, col in enumerate(dataframe.columns):\n",
    "        max_length = max(\n",
    "            dataframe[col].astype(str).str.len().max(),\n",
    "            len(str(col))\n",
    "        ) + 2\n",
    "        worksheet.column_dimensions[chr(65 + idx)].width = max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Extract Functions\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_alerts(from_date, to_date, lang=\"he\", retries=0):\n",
    "    \"\"\"\n",
    "    Fetch Red Alert data from the Oref API for a given date range with retry capability.\n",
    "    \n",
    "    Args:\n",
    "        from_date (datetime): Start date\n",
    "        to_date (datetime): End date\n",
    "        lang (str): Language code ('he' for Hebrew, 'en' for English)\n",
    "        retries (int): Current retry attempt\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the alert data\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"lang\": lang,\n",
    "        \"fromDate\": from_date.strftime(\"%d.%m.%Y\"),\n",
    "        \"toDate\": to_date.strftime(\"%d.%m.%Y\"),\n",
    "        \"mode\": \"0\"\n",
    "    }\n",
    "    \n",
    "    # Add custom user agent to avoid potential blocks\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"application/json, text/javascript, /; q=0.01\",\n",
    "        \"Accept-Language\": \"he-IL,he;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"Referer\": \"https://alerts-history.oref.org.il/\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Skip SSL verification with a warning\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
    "        \n",
    "        # Use session to maintain cookies\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # First visit the main site to get cookies\n",
    "        session.get(\"https://alerts-history.oref.org.il/\", verify=False, headers=headers)\n",
    "        \n",
    "        # Now make the actual API request\n",
    "        response = session.get(\n",
    "            OREF_API_URL, \n",
    "            params=params, \n",
    "            headers=headers,\n",
    "            verify=False,\n",
    "            timeout=15  # Set timeout to avoid hanging\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Explicitly set encoding to utf-8 before getting JSON content\n",
    "        response.encoding = 'utf-8'\n",
    "        data = response.json()\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "            if not df.empty:\n",
    "                print(f\"  Successfully fetched {len(df)} alerts.\")\n",
    "                \n",
    "                # Ensure proper encoding for text columns\n",
    "                for col in df.columns:\n",
    "                    if df[col].dtype == 'object':  # Only process string columns\n",
    "                        df[col] = df[col].apply(lambda x: x if not isinstance(x, str) else x.encode('latin1').decode('utf-8') if '\\\\u' in repr(x) else x)\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"  API returned empty list.\")\n",
    "        else:\n",
    "            print(f\"  Unexpected API response format: {type(data)}\")\n",
    "        \n",
    "        # If we reached here, we didn't get valid data\n",
    "        if retries < MAX_RETRIES:\n",
    "            print(f\"  Retrying ({retries+1}/{MAX_RETRIES}) after {RETRY_DELAY} seconds...\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "            return fetch_alerts(from_date, to_date, lang, retries+1)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  Error fetching data: {e}\")\n",
    "        if retries < MAX_RETRIES:\n",
    "            print(f\"  Retrying ({retries+1}/{MAX_RETRIES}) after {RETRY_DELAY} seconds...\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "            return fetch_alerts(from_date, to_date, lang, retries+1)\n",
    "    \n",
    "    # Return empty DataFrame with expected columns if all retries failed\n",
    "    return pd.DataFrame(columns=['data', 'date', 'time', 'category', 'title'])\n",
    "\n",
    "\n",
    "def fetch_alerts_in_chunks(start_date, end_date, initial_chunk_days=7, retry_days=3, delay_seconds=2):\n",
    "    \"\"\"\n",
    "    Fetch alerts in small chunks to avoid timeouts and API limitations.\n",
    "    Includes retry mechanism with smaller date ranges if initial chunks fail.\n",
    "    \n",
    "    Args:\n",
    "        start_date (datetime): Start date\n",
    "        end_date (datetime): End date\n",
    "        initial_chunk_days (int): Initial number of days in each chunk (default: 7 days)\n",
    "        retry_days (int): Number of days in each chunk for retry attempts (default: 3 days)\n",
    "        delay_seconds (int): Delay between API requests in seconds\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with all alerts\n",
    "    \"\"\"\n",
    "    all_alerts = []\n",
    "    chunks = get_date_range_chunks(start_date, end_date, initial_chunk_days)\n",
    "    all_unique_locations = set()\n",
    "    \n",
    "    for chunk_idx, (chunk_start, chunk_end) in enumerate(chunks):\n",
    "        # Add delay between requests to avoid overwhelming the API\n",
    "        if chunk_idx > 0:\n",
    "            print(f\"Waiting {delay_seconds} seconds before next request...\")\n",
    "            time.sleep(delay_seconds)\n",
    "        \n",
    "        print(f\"Fetching data from {chunk_start.strftime('%Y-%m-%d')} to {chunk_end.strftime('%Y-%m-%d')}...\")\n",
    "        chunk_df = fetch_alerts(chunk_start, chunk_end)\n",
    "        \n",
    "        # If initial chunk failed, try with smaller chunks\n",
    "        if chunk_df.empty and (chunk_end - chunk_start).days > retry_days:\n",
    "            print(f\"  No data found. Retrying with smaller chunks of {retry_days} days...\")\n",
    "            smaller_chunks = get_date_range_chunks(chunk_start, chunk_end, retry_days)\n",
    "            \n",
    "            for small_idx, (small_start, small_end) in enumerate(smaller_chunks):\n",
    "                # Add delay between smaller chunk requests\n",
    "                if small_idx > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                \n",
    "                print(f\"  Fetching smaller chunk: {small_start.strftime('%Y-%m-%d')} to {small_end.strftime('%Y-%m-%d')}...\")\n",
    "                small_df = fetch_alerts(small_start, small_end)\n",
    "                \n",
    "                if not small_df.empty:\n",
    "                    # Debug: Log unique locations in this chunk\n",
    "                    chunk_locations = set()\n",
    "                    for loc_str in small_df['data'].dropna():\n",
    "                        locations = [loc.strip() for loc in loc_str.split(',')]\n",
    "                        chunk_locations.update(locations)\n",
    "                    all_unique_locations.update(chunk_locations)\n",
    "                    \n",
    "                    print(f\"    Found {len(small_df)} alerts with {len(chunk_locations)} unique locations in this chunk.\")\n",
    "                    all_alerts.append(small_df)\n",
    "                else:\n",
    "                    print(f\"    No data found in smaller chunk.\")\n",
    "        \n",
    "        elif not chunk_df.empty:\n",
    "            # Debug: Log unique locations in this chunk\n",
    "            chunk_locations = set()\n",
    "            for loc_str in chunk_df['data'].dropna():\n",
    "                locations = [loc.strip() for loc in loc_str.split(',')]\n",
    "                chunk_locations.update(locations)\n",
    "            all_unique_locations.update(chunk_locations)\n",
    "            \n",
    "            print(f\"  Found {len(chunk_df)} alerts with {len(chunk_locations)} unique locations in this chunk.\")\n",
    "            all_alerts.append(chunk_df)\n",
    "        else:\n",
    "            print(f\"  No data found in this date range.\")\n",
    "    \n",
    "    if all_alerts:\n",
    "        # Ensure we're concatenating and getting the unique locations correctly\n",
    "        combined_df = pd.concat(all_alerts, ignore_index=True)\n",
    "        \n",
    "        # Show alert counts by date for verification\n",
    "        if not combined_df.empty and 'date' in combined_df.columns:\n",
    "            date_counts = combined_df.groupby('date').size()\n",
    "            print(\"\\nAlerts by date:\")\n",
    "            for date, count in date_counts.items():  # Changed from iteritems() to items()\n",
    "                print(f\"  {date}: {count} alerts\")\n",
    "                \n",
    "        # Count the unique locations in the final dataset\n",
    "        print(f\"\\nTotal combined unique locations across all chunks: {len(all_unique_locations)}\")\n",
    "        print(f\"Total alerts fetched: {len(combined_df)}\")\n",
    "        \n",
    "        # Save unique locations to file for inspection\n",
    "        with open('all_unique_locations.txt', 'w', encoding='utf-8') as f:\n",
    "            for loc in sorted(all_unique_locations):\n",
    "                f.write(f\"{loc}\\n\")\n",
    "        print(\"All unique locations saved to all_unique_locations.txt\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No alerts data found for the entire date range.\")\n",
    "        return pd.DataFrame(columns=['data', 'date', 'time', 'category', 'title'])\n",
    "\n",
    "\n",
    "def extract_unique_locations(alerts_df):\n",
    "    \"\"\"\n",
    "    Extract unique locations from the alerts dataframe.\n",
    "    \n",
    "    Args:\n",
    "        alerts_df (DataFrame): DataFrame containing alert data\n",
    "        \n",
    "    Returns:\n",
    "        list: List of unique location names\n",
    "    \"\"\"\n",
    "    # Check if 'data' column exists\n",
    "    if 'data' not in alerts_df.columns:\n",
    "        print(\"Warning: 'data' column not found in alerts DataFrame\")\n",
    "        return []\n",
    "        \n",
    "    unique_locations = set()\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Debug: Count total rows and non-null data entries\n",
    "    total_rows = len(alerts_df)\n",
    "    non_null_data = alerts_df['data'].count()\n",
    "    print(f\"Processing {non_null_data} non-null entries out of {total_rows} total rows\")\n",
    "    \n",
    "    # Process in smaller batches for better debugging\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(alerts_df), batch_size):\n",
    "        batch = alerts_df.iloc[i:i+batch_size]\n",
    "        batch_locations = set()\n",
    "        \n",
    "        for location_str in batch['data'].dropna():\n",
    "            if not isinstance(location_str, str):\n",
    "                # Skip non-string values\n",
    "                continue\n",
    "                \n",
    "            # Split by comma and clean whitespace\n",
    "            try:\n",
    "                locations = [loc.strip() for loc in location_str.split(',')]\n",
    "                batch_locations.update(locations)\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing location string '{location_str}': {e}\")\n",
    "        \n",
    "        unique_locations.update(batch_locations)\n",
    "        print(f\"Processed batch {i//batch_size + 1}, found {len(batch_locations)} unique locations in this batch\")\n",
    "    \n",
    "    print(f\"Processed {processed_count} location entries, found {len(unique_locations)} total unique locations\")\n",
    "    return sorted(unique_locations)\n",
    "\n",
    "\n",
    "def get_location_info(city, country=\"Israel\"):\n",
    "    \"\"\"\n",
    "    Get location information from OpenStreetMap for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): City name\n",
    "        country (str): Country name\n",
    "        \n",
    "    Returns:\n",
    "        dict: Location information including coordinates and region\n",
    "    \"\"\"\n",
    "    # Remove quotes if they exist and add country to the search\n",
    "    city_clean = clean_text(city)\n",
    "    \n",
    "    params = {\n",
    "        'q': f\"{city_clean}, {country}\",\n",
    "        'format': 'json',\n",
    "        'addressdetails': 1,\n",
    "        'limit': 1\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'RedAlertETL/1.0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Disable SSL verification with a warning\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
    "        \n",
    "        response = requests.get(NOMINATIM_API_URL, params=params, headers=headers, verify=False, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data:\n",
    "            result = data[0]\n",
    "            address = result.get('address', {})\n",
    "            \n",
    "            # Try to get the most specific region information\n",
    "            region = address.get('state',\n",
    "                     address.get('region',\n",
    "                     address.get('county',\n",
    "                     address.get('district', 'Unknown'))))\n",
    "            \n",
    "            return {\n",
    "                'city': city_clean,\n",
    "                'lat': float(result['lat']),\n",
    "                'lon': float(result['lon']),\n",
    "                'region': region,\n",
    "                'full_address': result.get('display_name', '')\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {str(e)}\")\n",
    "    \n",
    "    # Use region mapping if available for this location\n",
    "    if city_clean in MANUAL_REGION_MAPPINGS:\n",
    "        return {\n",
    "            'city': city_clean,\n",
    "            'lat': None,\n",
    "            'lon': None,\n",
    "            'region': MANUAL_REGION_MAPPINGS[city_clean],\n",
    "            'full_address': f\"{city_clean}, {country}\"\n",
    "        }\n",
    "    \n",
    "    # Return default values if geocoding fails\n",
    "    return {\n",
    "        'city': city_clean,\n",
    "        'lat': None,\n",
    "        'lon': None,\n",
    "        'region': 'UNKNOWN',\n",
    "        'full_address': None\n",
    "    }\n",
    "\n",
    "\n",
    "def batch_geocode_locations(locations, delay=1, max_errors=20):\n",
    "    \"\"\"\n",
    "    Geocode a batch of locations with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        locations (list): List of location names\n",
    "        delay (int): Delay between requests in seconds\n",
    "        max_errors (int): Maximum consecutive errors before falling back to mappings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with geocoding results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_locations = len(locations)\n",
    "    consecutive_errors = 0\n",
    "    use_manual_mapping = False\n",
    "    \n",
    "    # Add progress tracking\n",
    "    print(f\"Starting geocoding for {total_locations} locations...\")\n",
    "    \n",
    "    for idx, location in enumerate(locations, 1):\n",
    "        # Check if we've hit too many errors and should switch to manual mapping\n",
    "        if consecutive_errors >= max_errors and not use_manual_mapping:\n",
    "            print(f\"\\nWARNING: Encountered {max_errors} consecutive geocoding errors.\")\n",
    "            print(\"Switching to manual mapping for remaining locations.\")\n",
    "            use_manual_mapping = True\n",
    "        \n",
    "        # Print progress every 10 items\n",
    "        if idx % 10 == 0 or idx == 1 or idx == total_locations:\n",
    "            print(f\"Processing {idx}/{total_locations}: {location}\")\n",
    "        \n",
    "        # If we're in manual mapping mode, try to use that first\n",
    "        if use_manual_mapping and location in MANUAL_REGION_MAPPINGS:\n",
    "            result = {\n",
    "                'city': location,\n",
    "                'lat': None,\n",
    "                'lon': None,\n",
    "                'region': MANUAL_REGION_MAPPINGS[location],\n",
    "                'full_address': f\"{location}, Israel\"\n",
    "            }\n",
    "            results.append(result)\n",
    "            continue\n",
    "            \n",
    "        # Otherwise try geocoding\n",
    "        try:\n",
    "            result = get_location_info(location)\n",
    "            \n",
    "            # Check if geocoding failed (no lat/lon)\n",
    "            if result['lat'] is None or result['lon'] is None:\n",
    "                # Try manual mapping if available\n",
    "                if location in MANUAL_REGION_MAPPINGS:\n",
    "                    result['region'] = MANUAL_REGION_MAPPINGS[location]\n",
    "                    consecutive_errors = 0  # Reset error counter on successful manual mapping\n",
    "                else:\n",
    "                    consecutive_errors += 1\n",
    "            else:\n",
    "                consecutive_errors = 0  # Reset error counter on successful geocoding\n",
    "                \n",
    "            results.append(result)\n",
    "            time.sleep(delay)  # Respect rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing {location}: {str(e)}\")\n",
    "            consecutive_errors += 1\n",
    "            \n",
    "            # Add a fallback result\n",
    "            if location in MANUAL_REGION_MAPPINGS:\n",
    "                region = MANUAL_REGION_MAPPINGS[location]\n",
    "            else:\n",
    "                region = 'UNKNOWN'\n",
    "                \n",
    "            results.append({\n",
    "                'city': location,\n",
    "                'lat': None,\n",
    "                'lon': None,\n",
    "                'region': region,\n",
    "                'full_address': None\n",
    "            })\n",
    "            \n",
    "            # Add extra delay after an error\n",
    "            time.sleep(delay * 2)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    df = pd.DataFrame(results)\n",
    "    geocoded_count = df['lat'].notnull().sum()\n",
    "    manual_count = df['lat'].isnull().sum()\n",
    "    unknown_count = (df['region'] == 'UNKNOWN').sum()\n",
    "    \n",
    "    print(f\"\\nGeocoding summary:\")\n",
    "    print(f\"  Total locations: {total_locations}\")\n",
    "    print(f\"  Successfully geocoded: {geocoded_count} ({geocoded_count/total_locations*100:.1f}%)\")\n",
    "    print(f\"  Used manual mapping: {manual_count} ({manual_count/total_locations*100:.1f}%)\")\n",
    "    print(f\"  Unknown regions: {unknown_count} ({unknown_count/total_locations*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Transform Functions\n",
    "# =============================================================================\n",
    "\n",
    "def apply_manual_region_mapping(regions_df):\n",
    "    \"\"\"\n",
    "    Apply manual region mapping for locations that couldn't be geocoded.\n",
    "    \n",
    "    Args:\n",
    "        regions_df (DataFrame): DataFrame with location and region data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Updated DataFrame with manual mappings\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = regions_df.copy()\n",
    "    \n",
    "    # Update the region values in the DataFrame\n",
    "    for location, region in MANUAL_REGION_MAPPINGS.items():\n",
    "        result_df.loc[result_df['city'] == location, 'region'] = region\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def map_location_to_region(location, regions_mapping):\n",
    "    \"\"\"\n",
    "    Map a location to its region using the mapping dictionary.\n",
    "    \n",
    "    Args:\n",
    "        location (str): Location name\n",
    "        regions_mapping (DataFrame): DataFrame with city-region mappings\n",
    "        \n",
    "    Returns:\n",
    "        str: Region name\n",
    "    \"\"\"\n",
    "    # Split in case of multiple locations separated by comma\n",
    "    locations = [loc.strip() for loc in location.split(',')]\n",
    "    \n",
    "    for loc in locations:\n",
    "        # Try exact match\n",
    "        exact_match = regions_mapping[regions_mapping['city'] == loc]\n",
    "        if not exact_match.empty:\n",
    "            return exact_match.iloc[0]['region']\n",
    "        \n",
    "        # Try partial match\n",
    "        for city, region in regions_mapping[['city', 'region']].values:\n",
    "            if city in loc or loc in city:\n",
    "                return region\n",
    "    \n",
    "    return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def add_region_to_alerts(alerts_df, regions_mapping):\n",
    "    \"\"\"\n",
    "    Add region information to the alerts dataframe.\n",
    "    \n",
    "    Args:\n",
    "        alerts_df (DataFrame): DataFrame with alert data\n",
    "        regions_mapping (DataFrame): DataFrame with city-region mappings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Alert data with region information\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = alerts_df.copy()\n",
    "    \n",
    "    # Apply the mapping function to each row\n",
    "    result_df['region'] = result_df['data'].apply(\n",
    "        lambda x: map_location_to_region(x, regions_mapping)\n",
    "    )\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def translate_regions_to_english(df):\n",
    "    \"\"\"\n",
    "    Translate region names to English.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame with region column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added English region names\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add English region names\n",
    "    result_df['region_en'] = result_df['region'].map(REGION_TRANSLATION)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def filter_by_regions(df, regions):\n",
    "    \"\"\"\n",
    "    Filter dataframe by specified regions.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to filter\n",
    "        regions (list): List of region names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df['region_en'].isin(regions)]\n",
    "\n",
    "\n",
    "def filter_by_dates(df, dates):\n",
    "    \"\"\"\n",
    "    Filter dataframe by specified dates.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to filter\n",
    "        dates (list): List of date strings in 'YYYY-MM-DD' format\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Ensure date is in datetime format\n",
    "    if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')\n",
    "    \n",
    "    # Create a string version of the date for filtering\n",
    "    df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Filter and sort\n",
    "    filtered_df = df[df['date_str'].isin(dates)].sort_values(['date', 'time'])\n",
    "    \n",
    "    return filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Functions\n",
    "# =============================================================================\n",
    "\n",
    "def save_to_csv(df, filepath, encoding=None, index=False):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        filepath (str): Output file path\n",
    "        encoding (str, optional): File encoding\n",
    "        index (bool): Whether to include index in output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use UTF-8 with BOM for Hebrew text\n",
    "        encoding = 'utf-8-sig'\n",
    "        \n",
    "        # Convert any problematic text in string columns\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':  # Only process string columns\n",
    "                df[col] = df[col].apply(lambda x: x if not isinstance(x, str) else x)\n",
    "        \n",
    "        df.to_csv(filepath, index=index, encoding=encoding)\n",
    "        print(f\"Data saved to CSV: {filepath} with encoding {encoding}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {e}\")\n",
    "        \n",
    "        # Try alternative encoding if primary fails\n",
    "        try:\n",
    "            df.to_csv(filepath, index=index, encoding='cp1255')  # Windows Hebrew encoding\n",
    "            print(f\"Data saved to CSV with alternative encoding (cp1255): {filepath}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to save CSV with alternative encoding: {e2}\")\n",
    "\n",
    "\n",
    "def save_to_excel(df, filepath, sheet_name='Data', index=False, optimize_columns=True):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Excel file with optimized column widths.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        filepath (str): Output file path\n",
    "        sheet_name (str): Name of the sheet\n",
    "        index (bool): Whether to include index in output\n",
    "        optimize_columns (bool): Whether to optimize column widths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fix Hebrew encoding issues in string columns\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':  # Only process string columns\n",
    "                df[col] = df[col].apply(lambda x: x if not isinstance(x, str) else x)\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine=EXCEL_ENGINE) as writer:\n",
    "            df.to_excel(writer, index=index, sheet_name=sheet_name)\n",
    "            \n",
    "            if optimize_columns:\n",
    "                try:\n",
    "                    worksheet = writer.sheets[sheet_name]\n",
    "                    optimize_excel_columns(worksheet, df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not optimize columns: {e}\")\n",
    "        \n",
    "        print(f\"Data saved to Excel: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "        \n",
    "        # Try alternative approach if primary fails\n",
    "        try:\n",
    "            # Simplified saving without optimization\n",
    "            df.to_excel(filepath, index=index, sheet_name=sheet_name, engine=EXCEL_ENGINE)\n",
    "            print(f\"Data saved to Excel with simplified method: {filepath}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to save Excel with alternative method: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "          Red Alert ETL Pipeline\n",
      "============================================================\n",
      "This pipeline extracts alerts data from פיקוד העורף (Oref)\n",
      "processes it, and exports filtered data for analysis.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Extract Stage ===\n",
      "Skipping extraction, loading existing files...\n",
      "Loaded 9791 alerts and 878 locations\n",
      "\n",
      "=== Transform Stage ===\n",
      "Skipping transformation, loading existing files...\n",
      "Error: central_regions_66days_sorted.xlsx not found. Cannot skip to analysis.\n",
      "Applying manual region mappings...\n",
      "Data saved to Excel: israel_cities_regions_updated.xlsx\n",
      "Adding region information to alerts...\n",
      "Translating regions to English...\n",
      "Filtering by central regions...\n",
      "Filtering by specific dates...\n",
      "\n",
      "=== Load Stage ===\n",
      "Saving alerts with regions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hagai-BY\\AppData\\Local\\Temp\\ipykernel_9344\\2354315969.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')\n",
      "C:\\Users\\Hagai-BY\\AppData\\Local\\Temp\\ipykernel_9344\\2354315969.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Excel: alarms_with_regions.xlsx\n",
      "Saving alerts with English regions...\n",
      "Data saved to Excel: alarms_with_english_regions.xlsx\n",
      "Saving central region alerts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hagai-BY\\AppData\\Local\\Temp\\ipykernel_9344\\2179987688.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].apply(lambda x: x if not isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Excel: central_regions_alarms.xlsx\n",
      "Saving final filtered alerts...\n",
      "Data saved to Excel: central_regions_66days_sorted.xlsx\n",
      "\n",
      "=== Analysis Stage ===\n",
      "\n",
      "=== Analysis ===\n",
      "Alert counts by date:\n",
      "date\n",
      "2023-10-08     26\n",
      "2023-10-09     65\n",
      "2023-10-10     46\n",
      "2023-10-11     40\n",
      "2023-10-12     14\n",
      "2023-10-15     36\n",
      "2023-10-16     69\n",
      "2023-10-17    134\n",
      "2023-10-18     18\n",
      "2023-10-19     24\n",
      "2023-10-22      3\n",
      "2023-10-24    128\n",
      "2023-10-25     25\n",
      "2023-10-26    106\n",
      "2023-10-29     16\n",
      "2023-10-30     40\n",
      "2023-10-31     47\n",
      "2023-11-01     29\n",
      "2023-11-02     30\n",
      "2023-11-05     75\n",
      "2023-11-06      1\n",
      "2023-11-07     51\n",
      "2023-11-09      2\n",
      "2023-11-13     24\n",
      "2023-11-14      5\n",
      "2023-11-15      1\n",
      "2023-11-20    137\n",
      "2023-11-21     14\n",
      "2023-12-03      1\n",
      "2023-12-04     11\n",
      "2023-12-05     36\n",
      "2023-12-11     16\n",
      "2023-12-13      4\n",
      "2023-12-19     59\n",
      "2023-12-21     86\n",
      "dtype: int64\n",
      "\n",
      "Alert counts by region:\n",
      "region_en\n",
      "CENTRAL_DISTRICT     1075\n",
      "TEL_AVIV_DISTRICT     280\n",
      "DAN_DISTRICT           44\n",
      "SHARON_DISTRICT        20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total alerts: 1419\n",
      "Days with alerts: 35 out of 66 filtered days\n",
      "Average alerts per day: 40.54\n",
      "\n",
      "Dates without alerts:\n",
      "2023-10-01\n",
      "2023-10-02\n",
      "2023-10-03\n",
      "2023-10-04\n",
      "2023-10-05\n",
      "2023-10-23\n",
      "2023-11-08\n",
      "2023-11-12\n",
      "2023-11-16\n",
      "2023-11-19\n",
      "2023-11-22\n",
      "2023-11-23\n",
      "2023-11-26\n",
      "2023-11-27\n",
      "2023-11-28\n",
      "2023-11-29\n",
      "2023-11-30\n",
      "2023-12-06\n",
      "2023-12-07\n",
      "2023-12-10\n",
      "2023-12-12\n",
      "2023-12-14\n",
      "2023-12-17\n",
      "2023-12-18\n",
      "2023-12-20\n",
      "2023-12-24\n",
      "2023-12-25\n",
      "2023-12-26\n",
      "2023-12-27\n",
      "2023-12-28\n",
      "2023-12-31\n",
      "\n",
      "ETL Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Main ETL Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def extract_stage():\n",
    "    \"\"\"Extract data from sources\"\"\"\n",
    "    print(\"\\n=== Extract Stage ===\")\n",
    "    \n",
    "    # Define a default locations list in case we can't get real data\n",
    "    # This is a fallback to avoid errors when API calls fail\n",
    "    default_locations = [\n",
    "        'אשקלון', 'אשדוד', 'תל אביב', 'באר שבע', 'ירושלים',\n",
    "        'חיפה', 'נתניה', 'רעננה', 'הרצליה', 'פתח תקווה'\n",
    "    ]\n",
    "    \n",
    "    # Check if user wants to force fetch new data\n",
    "    force_fetch = False\n",
    "    user_input = input(\"Do you want to fetch fresh data from the API? (y/n, default: n): \").strip().lower()\n",
    "    if user_input == 'y' or user_input == 'yes':\n",
    "        force_fetch = True\n",
    "        # Force delete old file to avoid any mixing with previous data\n",
    "        if os.path.exists(MERGED_ALARMS_PATH):\n",
    "            try:\n",
    "                os.remove(MERGED_ALARMS_PATH)\n",
    "                print(f\"Deleted existing data file: {MERGED_ALARMS_PATH}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete existing file: {e}\")\n",
    "    \n",
    "    # Check if merged file already exists to avoid re-fetching\n",
    "    if os.path.exists(MERGED_ALARMS_PATH) and not force_fetch:\n",
    "        print(f\"Loading existing data from {MERGED_ALARMS_PATH}\")\n",
    "        try:\n",
    "            # Try multiple encodings for Hebrew text\n",
    "            try:\n",
    "                alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8-sig')\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='cp1255')\n",
    "            \n",
    "            # Fix any encoding issues in text columns\n",
    "            for col in alerts_df.columns:\n",
    "                if alerts_df[col].dtype == 'object':  # Only process string columns\n",
    "                    alerts_df[col] = alerts_df[col].apply(lambda x: x if not isinstance(x, str) \n",
    "                                                        else x.encode('latin1').decode('utf-8') \n",
    "                                                        if '\\\\u' in repr(x) else x)\n",
    "        \n",
    "            # Verify the dataframe has the expected structure\n",
    "            if 'data' not in alerts_df.columns:\n",
    "                print(\"Warning: CSV file does not have expected structure\")\n",
    "                if not force_fetch:\n",
    "                    user_input = input(\"CSV file has invalid structure. Fetch new data? (y/n, default: y): \").strip().lower()\n",
    "                    if user_input != 'n' and user_input != 'no':\n",
    "                        force_fetch = True\n",
    "                \n",
    "                if not force_fetch:\n",
    "                    # Create a minimal dataframe with required columns\n",
    "                    alerts_df = pd.DataFrame({\n",
    "                        'data': default_locations,\n",
    "                        'date': [datetime.now().strftime('%d.%m.%Y')] * len(default_locations),\n",
    "                        'time': ['00:00:00'] * len(default_locations),\n",
    "                        'category': [''] * len(default_locations),\n",
    "                        'title': [''] * len(default_locations)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the CSV file: {e}\")\n",
    "            force_fetch = True\n",
    "    else:\n",
    "        force_fetch = True\n",
    "    \n",
    "    # Fetch new data if needed or requested\n",
    "    if force_fetch:\n",
    "        print(\"Fetching alerts data from API...\")\n",
    "        \n",
    "        # Allow user to customize chunk size\n",
    "        chunk_size = 7  # Default 7 days\n",
    "        try:\n",
    "            user_chunk = input(\"Enter chunk size in days (default: 7): \").strip()\n",
    "            if user_chunk and user_chunk.isdigit():\n",
    "                chunk_size = int(user_chunk)\n",
    "                if chunk_size < 1 or chunk_size > 30:\n",
    "                    print(\"Invalid chunk size. Using default (7 days).\")\n",
    "                    chunk_size = 7\n",
    "        except:\n",
    "            print(\"Invalid input. Using default chunk size (7 days).\")\n",
    "        \n",
    "        # Custom date range if needed\n",
    "        use_custom_dates = False\n",
    "        custom_start = START_DATE\n",
    "        custom_end = END_DATE\n",
    "        \n",
    "        user_input = input(\"Use custom date range instead of defaults? (y/n, default: n): \").strip().lower()\n",
    "        if user_input == 'y' or user_input == 'yes':\n",
    "            use_custom_dates = True\n",
    "            try:\n",
    "                start_date_str = input(f\"Enter start date (YYYY-MM-DD, default: {START_DATE.strftime('%Y-%m-%d')}): \").strip()\n",
    "                if start_date_str:\n",
    "                    custom_start = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "                \n",
    "                end_date_str = input(f\"Enter end date (YYYY-MM-DD, default: {END_DATE.strftime('%Y-%m-%d')}): \").strip()\n",
    "                if end_date_str:\n",
    "                    custom_end = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "                \n",
    "                print(f\"Using custom date range: {custom_start.strftime('%Y-%m-%d')} to {custom_end.strftime('%Y-%m-%d')}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Invalid date format: {e}. Using default date range.\")\n",
    "                use_custom_dates = False\n",
    "                \n",
    "        # Special handling for Oct 7-8 high volume period\n",
    "        special_processing = False\n",
    "        user_input = input(\"Use special processing for high-volume days (Oct 7-8, 2023)? (y/n, default: n): \").strip().lower()\n",
    "        if user_input == 'y' or user_input == 'yes':\n",
    "            special_processing = True\n",
    "            \n",
    "        # Initialize with empty dataframe\n",
    "        alerts_df = pd.DataFrame(columns=['data', 'date', 'time', 'category', 'title'])\n",
    "        all_dfs = []\n",
    "            \n",
    "        if special_processing:\n",
    "            # Oct 7-8 special processing with smaller time chunks\n",
    "            oct7_start = datetime(2023, 10, 7, 0, 0)\n",
    "            oct8_end = datetime(2023, 10, 9, 0, 0)  # End of Oct 8\n",
    "            \n",
    "            oct7_8_df = fetch_high_volume_days(oct7_start, oct8_end, hours_per_chunk=6)\n",
    "            \n",
    "            if not oct7_8_df.empty:\n",
    "                all_dfs.append(oct7_8_df)\n",
    "                \n",
    "            # Modify the date range to exclude Oct 7-8 if we're fetching the full range\n",
    "            if custom_start <= oct7_start and custom_end >= oct8_end:\n",
    "                # Process before Oct 7\n",
    "                if custom_start < oct7_start:\n",
    "                    before_df = fetch_alerts_in_chunks(\n",
    "                        custom_start, \n",
    "                        oct7_start - timedelta(days=1), \n",
    "                        initial_chunk_days=chunk_size, \n",
    "                        retry_days=max(1, chunk_size // 3)\n",
    "                    )\n",
    "                    if not before_df.empty:\n",
    "                        all_dfs.append(before_df)\n",
    "                \n",
    "                # Process after Oct 8\n",
    "                if custom_end > oct8_end:\n",
    "                    after_df = fetch_alerts_in_chunks(\n",
    "                        oct8_end + timedelta(days=1),\n",
    "                        custom_end,\n",
    "                        initial_chunk_days=chunk_size, \n",
    "                        retry_days=max(1, chunk_size // 3)\n",
    "                    )\n",
    "                    if not after_df.empty:\n",
    "                        all_dfs.append(after_df)\n",
    "            else:\n",
    "                # If we're not covering Oct 7-8 with our custom range, process normally\n",
    "                normal_df = fetch_alerts_in_chunks(\n",
    "                    custom_start if use_custom_dates else START_DATE, \n",
    "                    custom_end if use_custom_dates else END_DATE, \n",
    "                    initial_chunk_days=chunk_size, \n",
    "                    retry_days=max(1, chunk_size // 3)\n",
    "                )\n",
    "                if not normal_df.empty:\n",
    "                    all_dfs.append(normal_df)\n",
    "        else:\n",
    "            # Normal processing for the entire date range\n",
    "            normal_df = fetch_alerts_in_chunks(\n",
    "                custom_start if use_custom_dates else START_DATE, \n",
    "                custom_end if use_custom_dates else END_DATE, \n",
    "                initial_chunk_days=chunk_size, \n",
    "                retry_days=max(1, chunk_size // 3)\n",
    "            )\n",
    "            if not normal_df.empty:\n",
    "                all_dfs.append(normal_df)\n",
    "        \n",
    "        # Combine all data chunks\n",
    "        if all_dfs:\n",
    "            alerts_df = pd.concat(all_dfs, ignore_index=True)\n",
    "            \n",
    "            # Remove any duplicates that might have occurred in overlapping fetches\n",
    "            before_dedup = len(alerts_df)\n",
    "            alerts_df = alerts_df.drop_duplicates()\n",
    "            after_dedup = len(alerts_df)\n",
    "            if before_dedup > after_dedup:\n",
    "                print(f\"Removed {before_dedup - after_dedup} duplicate alerts\")\n",
    "        \n",
    "        # If we didn't get any data, create a minimal dataframe\n",
    "        if alerts_df.empty or 'data' not in alerts_df.columns:\n",
    "            print(\"Warning: No data retrieved from API, using sample data\")\n",
    "            alerts_df = pd.DataFrame({\n",
    "                'data': default_locations,\n",
    "                'date': [datetime.now().strftime('%d.%m.%Y')] * len(default_locations),\n",
    "                'time': ['00:00:00'] * len(default_locations),\n",
    "                'category': [''] * len(default_locations),\n",
    "                'title': [''] * len(default_locations)\n",
    "            })\n",
    "        else:\n",
    "            # Display debug information\n",
    "            print(\"\\nData statistics:\")\n",
    "            print(f\"Total alerts fetched: {len(alerts_df)}\")\n",
    "            unique_locations_count = extract_unique_locations(alerts_df)\n",
    "            print(f\"Unique locations found: {len(unique_locations_count)}\")\n",
    "            print(f\"Date range in data: {alerts_df['date'].min()} to {alerts_df['date'].max()}\")\n",
    "            \n",
    "        # Save with a timestamp to ensure we don't mix data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        backup_path = f\"merged_alarms_data_{timestamp}.csv\"\n",
    "        save_to_csv(alerts_df, backup_path)\n",
    "        print(f\"Backup saved to: {backup_path}\")\n",
    "        \n",
    "        # Save to main file\n",
    "        save_to_csv(alerts_df, MERGED_ALARMS_PATH)\n",
    "    \n",
    "    # Always extract and display unique locations count from current data\n",
    "    unique_locs = extract_unique_locations(alerts_df)\n",
    "    print(f\"\\nCurrent unique locations count: {len(unique_locs)}\")\n",
    "    \n",
    "    # Check if city regions file already exists\n",
    "    if os.path.exists(CITIES_REGIONS_PATH):\n",
    "        print(f\"Loading existing city region data from {CITIES_REGIONS_PATH}\")\n",
    "        city_regions_df = pd.read_excel(CITIES_REGIONS_PATH)\n",
    "    else:\n",
    "        print(\"Extracting unique locations...\")\n",
    "        unique_locations = extract_unique_locations(alerts_df)\n",
    "        print(f\"Found {len(unique_locations)} unique locations\")\n",
    "        \n",
    "        print(\"Geocoding locations...\")\n",
    "        city_regions_df = batch_geocode_locations(unique_locations)\n",
    "        save_to_excel(city_regions_df, CITIES_REGIONS_PATH)\n",
    "    \n",
    "    return alerts_df, city_regions_df\n",
    "\n",
    "\n",
    "def transform_stage(alerts_df, city_regions_df):\n",
    "    \"\"\"Transform and enrich the data\"\"\"\n",
    "    print(\"\\n=== Transform Stage ===\")\n",
    "    \n",
    "    # Apply manual region mapping for missing locations\n",
    "    print(\"Applying manual region mappings...\")\n",
    "    updated_regions_df = apply_manual_region_mapping(city_regions_df)\n",
    "    save_to_excel(updated_regions_df, CITIES_REGIONS_UPDATED_PATH)\n",
    "    \n",
    "    # Add region information to alerts\n",
    "    print(\"Adding region information to alerts...\")\n",
    "    alerts_with_regions = add_region_to_alerts(alerts_df, updated_regions_df)\n",
    "    \n",
    "    # Translate regions to English\n",
    "    print(\"Translating regions to English...\")\n",
    "    alerts_with_english = translate_regions_to_english(alerts_with_regions)\n",
    "    \n",
    "    # Filter by central regions\n",
    "    print(\"Filtering by central regions...\")\n",
    "    central_alerts = filter_by_regions(alerts_with_english, CENTRAL_REGIONS)\n",
    "    \n",
    "    # Filter by specific dates\n",
    "    print(\"Filtering by specific dates...\")\n",
    "    filtered_alerts = filter_by_dates(central_alerts, FILTER_DATES)\n",
    "    \n",
    "    # Remove temporary columns used for filtering\n",
    "    if 'date_str' in filtered_alerts.columns:\n",
    "        filtered_alerts = filtered_alerts.drop('date_str', axis=1)\n",
    "    \n",
    "    return alerts_with_regions, alerts_with_english, central_alerts, filtered_alerts\n",
    "\n",
    "\n",
    "def load_stage(alerts_with_regions, alerts_with_english, central_alerts, filtered_alerts):\n",
    "    \"\"\"Load processed data to files\"\"\"\n",
    "    print(\"\\n=== Load Stage ===\")\n",
    "    \n",
    "    print(\"Saving alerts with regions...\")\n",
    "    save_to_excel(alerts_with_regions, ALARMS_WITH_REGIONS_PATH)\n",
    "    \n",
    "    print(\"Saving alerts with English regions...\")\n",
    "    save_to_excel(alerts_with_english, ALARMS_WITH_ENGLISH_REGIONS_PATH)\n",
    "    \n",
    "    print(\"Saving central region alerts...\")\n",
    "    save_to_excel(central_alerts, CENTRAL_REGIONS_ALARMS_PATH)\n",
    "    \n",
    "    print(\"Saving final filtered alerts...\")\n",
    "    save_to_excel(\n",
    "        filtered_alerts, \n",
    "        FINAL_FILTERED_ALARMS_PATH,\n",
    "        sheet_name='Filtered Alarms'\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_results(filtered_alerts):\n",
    "    \"\"\"Perform basic analysis on the final dataset\"\"\"\n",
    "    print(\"\\n=== Analysis ===\")\n",
    "    \n",
    "    # Count alerts by date\n",
    "    date_counts = filtered_alerts.groupby(\n",
    "        filtered_alerts['date'].dt.strftime('%Y-%m-%d')\n",
    "    ).size().sort_index()\n",
    "    \n",
    "    print(\"Alert counts by date:\")\n",
    "    print(date_counts)\n",
    "    \n",
    "    # Count alerts by region\n",
    "    region_counts = filtered_alerts['region_en'].value_counts()\n",
    "    print(\"\\nAlert counts by region:\")\n",
    "    print(region_counts)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_alerts = len(filtered_alerts)\n",
    "    total_days = len(date_counts)\n",
    "    avg_alerts_per_day = total_alerts / total_days if total_days > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTotal alerts: {total_alerts}\")\n",
    "    print(f\"Days with alerts: {total_days} out of {len(FILTER_DATES)} filtered days\")\n",
    "    print(f\"Average alerts per day: {avg_alerts_per_day:.2f}\")\n",
    "    \n",
    "    # Find missing dates\n",
    "    all_dates_set = set(FILTER_DATES)\n",
    "    dates_with_alerts = set(date_counts.index)\n",
    "    missing_dates = all_dates_set - dates_with_alerts\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(\"\\nDates without alerts:\")\n",
    "        for date in sorted(missing_dates):\n",
    "            print(date)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL pipeline execution\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"          Red Alert ETL Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This pipeline extracts alerts data from פיקוד העורף (Oref)\")\n",
    "    print(\"processes it, and exports filtered data for analysis.\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Force clean start with diagnostics\n",
    "        force_clean = False\n",
    "        user_input = input(\"DIAGNOSTIC MODE: Force completely clean start? (y/n, default: n): \").strip().lower()\n",
    "        if user_input == 'y' or user_input == 'yes':\n",
    "            force_clean = True\n",
    "            print(\"\\n=== DIAGNOSTIC: Cleaning all data files ===\")\n",
    "            \n",
    "            # List of files to potentially delete for a clean start\n",
    "            data_files = [\n",
    "                MERGED_ALARMS_PATH,\n",
    "                CITIES_REGIONS_PATH,\n",
    "                CITIES_REGIONS_UPDATED_PATH,\n",
    "                ALARMS_WITH_REGIONS_PATH,\n",
    "                ALARMS_WITH_ENGLISH_REGIONS_PATH,\n",
    "                CENTRAL_REGIONS_ALARMS_PATH,\n",
    "                FINAL_FILTERED_ALARMS_PATH\n",
    "            ]\n",
    "            \n",
    "            for file_path in data_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"  Deleted: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error deleting {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"  File does not exist: {file_path}\")\n",
    "            \n",
    "            print(\"=== Diagnostic cleaning completed ===\\n\")\n",
    "        \n",
    "        # Allow skipping stages for faster debugging/reprocessing\n",
    "        start_at_transform = False\n",
    "        skip_to_analysis = False\n",
    "        \n",
    "        if not force_clean:\n",
    "            user_input = input(\"Start at transform stage? (y/n, default: n): \").strip().lower()\n",
    "            if user_input == 'y' or user_input == 'yes':\n",
    "                start_at_transform = True\n",
    "            \n",
    "            user_input = input(\"Skip to analysis stage? (y/n, default: n): \").strip().lower()\n",
    "            if user_input == 'y' or user_input == 'yes':\n",
    "                skip_to_analysis = True\n",
    "                start_at_transform = True\n",
    "        \n",
    "        # Define variables that might be needed later\n",
    "        alerts_df = None\n",
    "        city_regions_df = None\n",
    "        alerts_with_regions = None\n",
    "        alerts_with_english = None\n",
    "        central_alerts = None\n",
    "        filtered_alerts = None\n",
    "        \n",
    "        # Extract stage\n",
    "        if not start_at_transform:\n",
    "            print(\"\\n=== Extract Stage ===\")\n",
    "            \n",
    "            # Check if user wants to force fetch new data\n",
    "            force_fetch = False\n",
    "            user_input = input(\"Do you want to fetch fresh data from the API? (y/n, default: n): \").strip().lower()\n",
    "            if user_input == 'y' or user_input == 'yes':\n",
    "                force_fetch = True\n",
    "                # Force delete old file to avoid any mixing with previous data\n",
    "                if os.path.exists(MERGED_ALARMS_PATH):\n",
    "                    try:\n",
    "                        os.remove(MERGED_ALARMS_PATH)\n",
    "                        print(f\"Deleted existing data file: {MERGED_ALARMS_PATH}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not delete existing file: {e}\")\n",
    "            \n",
    "            # Check if merged file already exists to avoid re-fetching\n",
    "            if os.path.exists(MERGED_ALARMS_PATH) and not force_fetch:\n",
    "                print(f\"Loading existing data from {MERGED_ALARMS_PATH}\")\n",
    "                try:\n",
    "                    # Try multiple encodings for Hebrew text\n",
    "                    try:\n",
    "                        alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8-sig')\n",
    "                    except UnicodeDecodeError:\n",
    "                        try:\n",
    "                            alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8')\n",
    "                        except UnicodeDecodeError:\n",
    "                            alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='cp1255')\n",
    "                    \n",
    "                    # Verify the dataframe has the expected structure\n",
    "                    if 'data' not in alerts_df.columns:\n",
    "                        print(\"Warning: CSV file does not have expected structure\")\n",
    "                        force_fetch = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading the CSV file: {e}\")\n",
    "                    force_fetch = True\n",
    "            else:\n",
    "                force_fetch = True\n",
    "            \n",
    "            # Fetch new data if needed or requested\n",
    "            if force_fetch:\n",
    "                print(\"Fetching alerts data from API...\")\n",
    "                \n",
    "                # Allow user to customize chunk size\n",
    "                chunk_size = 7  # Default 7 days\n",
    "                try:\n",
    "                    user_chunk = input(\"Enter chunk size in days (default: 7): \").strip()\n",
    "                    if user_chunk and user_chunk.isdigit():\n",
    "                        chunk_size = int(user_chunk)\n",
    "                        if chunk_size < 1 or chunk_size > 30:\n",
    "                            print(\"Invalid chunk size. Using default (7 days).\")\n",
    "                            chunk_size = 7\n",
    "                except:\n",
    "                    print(\"Invalid input. Using default chunk size (7 days).\")\n",
    "                \n",
    "                # Custom date range if needed\n",
    "                use_custom_dates = False\n",
    "                custom_start = START_DATE\n",
    "                custom_end = END_DATE\n",
    "                \n",
    "                user_input = input(\"Use custom date range instead of defaults? (y/n, default: n): \").strip().lower()\n",
    "                if user_input == 'y' or user_input == 'yes':\n",
    "                    use_custom_dates = True\n",
    "                    try:\n",
    "                        start_date_str = input(f\"Enter start date (YYYY-MM-DD, default: {START_DATE.strftime('%Y-%m-%d')}): \").strip()\n",
    "                        if start_date_str:\n",
    "                            custom_start = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "                        \n",
    "                        end_date_str = input(f\"Enter end date (YYYY-MM-DD, default: {END_DATE.strftime('%Y-%m-%d')}): \").strip()\n",
    "                        if end_date_str:\n",
    "                            custom_end = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "                        \n",
    "                        print(f\"Using custom date range: {custom_start.strftime('%Y-%m-%d')} to {custom_end.strftime('%Y-%m-%d')}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Invalid date format: {e}. Using default date range.\")\n",
    "                        use_custom_dates = False\n",
    "                \n",
    "                # Initialize with empty dataframe\n",
    "                all_dfs = []\n",
    "                \n",
    "                # Special handling for Oct 7-8 high volume period\n",
    "                process_high_volume = False\n",
    "                user_input = input(\"Process high-volume days (Oct 7-8, 2023)? (y/n, default: n): \").strip().lower()\n",
    "                if user_input == 'y' or user_input == 'yes':\n",
    "                    process_high_volume = True\n",
    "                    \n",
    "                # Define high-volume period dates\n",
    "                oct7_start = datetime(2023, 10, 7, 0, 0)\n",
    "                oct8_end = datetime(2023, 10, 9, 0, 0)  # End of Oct 8\n",
    "                \n",
    "                # Determine effective date range to process\n",
    "                effective_start = custom_start if use_custom_dates else START_DATE\n",
    "                effective_end = custom_end if use_custom_dates else END_DATE\n",
    "                \n",
    "                # Process data in a way that ensures proper integration\n",
    "                if process_high_volume and effective_start <= oct8_end and effective_end >= oct7_start:\n",
    "                    print(\"\\n--- Processing High Volume Period First ---\")\n",
    "                    \n",
    "                    # Determine the overlap between requested range and high-volume period\n",
    "                    hv_start = max(effective_start, oct7_start)\n",
    "                    hv_end = min(effective_end, oct8_end)\n",
    "                    \n",
    "                    print(f\"Fetching high-volume period: {hv_start.strftime('%Y-%m-%d %H:%M')} - {hv_end.strftime('%Y-%m-%d %H:%M')}\")\n",
    "                    high_volume_df = fetch_high_volume_days(hv_start, hv_end, hours_per_chunk=6)\n",
    "                    \n",
    "                    if not high_volume_df.empty:\n",
    "                        print(f\"Successfully fetched {len(high_volume_df)} alerts for high-volume period\")\n",
    "                        all_dfs.append(high_volume_df)\n",
    "                    else:\n",
    "                        print(\"No data found for high-volume period\")\n",
    "                    \n",
    "                    # Process data before high-volume period if needed\n",
    "                    if effective_start < oct7_start:\n",
    "                        print(\"\\n--- Processing Period Before High Volume Days ---\")\n",
    "                        print(f\"Fetching data from {effective_start.strftime('%Y-%m-%d')} to {(oct7_start - timedelta(days=1)).strftime('%Y-%m-%d')}...\")\n",
    "                        \n",
    "                        before_df = fetch_alerts_in_chunks(\n",
    "                            effective_start,\n",
    "                            oct7_start - timedelta(days=1),\n",
    "                            initial_chunk_days=chunk_size,\n",
    "                            retry_days=max(1, chunk_size // 3)\n",
    "                        )\n",
    "                        \n",
    "                        if not before_df.empty:\n",
    "                            print(f\"Successfully fetched {len(before_df)} alerts for period before high-volume days\")\n",
    "                            all_dfs.append(before_df)\n",
    "                        else:\n",
    "                            print(\"No data found for period before high-volume days\")\n",
    "                    \n",
    "                    # Process data after high-volume period if needed\n",
    "                    if effective_end > oct8_end:\n",
    "                        print(\"\\n--- Processing Period After High Volume Days ---\")\n",
    "                        # MODIFIED LINE: Start from oct8_end which is 2023-10-09 00:00\n",
    "                        # This includes Oct 9 in the fetch\n",
    "                        print(f\"Fetching data from {oct8_end.strftime('%Y-%m-%d')} to {effective_end.strftime('%Y-%m-%d')}...\")\n",
    "                        \n",
    "                        # MODIFIED LINE: Removed the +timedelta(days=1) to start from \n",
    "                        # exactly where the high-volume period ended (Oct 9 00:00)\n",
    "                        after_df = fetch_alerts_in_chunks(\n",
    "                            oct8_end,  # This is 2023-10-09 00:00\n",
    "                            effective_end,\n",
    "                            initial_chunk_days=chunk_size,\n",
    "                            retry_days=max(1, chunk_size // 3)\n",
    "                        )\n",
    "                        \n",
    "                        if not after_df.empty:\n",
    "                            print(f\"Successfully fetched {len(after_df)} alerts for period after high-volume days\")\n",
    "                            all_dfs.append(after_df)\n",
    "                        else:\n",
    "                            print(\"No data found for period after high-volume days\")\n",
    "                \n",
    "                else:\n",
    "                    # Normal processing (no high-volume days or they're not in our range)\n",
    "                    print(\"\\n--- Processing Entire Date Range Normally ---\")\n",
    "                    print(f\"Fetching data from {effective_start.strftime('%Y-%m-%d')} to {effective_end.strftime('%Y-%m-%d')}...\")\n",
    "                    \n",
    "                    normal_df = fetch_alerts_in_chunks(\n",
    "                        effective_start,\n",
    "                        effective_end,\n",
    "                        initial_chunk_days=chunk_size,\n",
    "                        retry_days=max(1, chunk_size // 3)\n",
    "                    )\n",
    "                    \n",
    "                    if not normal_df.empty:\n",
    "                        print(f\"Successfully fetched {len(normal_df)} alerts for the entire period\")\n",
    "                        all_dfs.append(normal_df)\n",
    "                    else:\n",
    "                        print(\"No data found for the requested period\")\n",
    "                \n",
    "                # Combine all data chunks\n",
    "                if all_dfs:\n",
    "                    print(\"\\n--- Combining All Data ---\")\n",
    "                    alerts_df = pd.concat(all_dfs, ignore_index=True)\n",
    "                    \n",
    "                    # Remove any duplicates that might have occurred in overlapping fetches\n",
    "                    before_dedup = len(alerts_df)\n",
    "                    alerts_df = alerts_df.drop_duplicates()\n",
    "                    after_dedup = len(alerts_df)\n",
    "                    if before_dedup > after_dedup:\n",
    "                        print(f\"Removed {before_dedup - after_dedup} duplicate alerts\")\n",
    "                    \n",
    "                    # Display data statistics\n",
    "                    print(\"\\nData statistics:\")\n",
    "                    print(f\"Total alerts fetched: {len(alerts_df)}\")\n",
    "                    \n",
    "                    if 'date' in alerts_df.columns:\n",
    "                        # Convert to datetime for easier manipulation if not already\n",
    "                        if not pd.api.types.is_datetime64_any_dtype(alerts_df['date']):\n",
    "                            try:\n",
    "                                alerts_df['date_dt'] = pd.to_datetime(alerts_df['date'], format='%d.%m.%Y')\n",
    "                                date_min = alerts_df['date_dt'].min().strftime('%Y-%m-%d')\n",
    "                                date_max = alerts_df['date_dt'].max().strftime('%Y-%m-%d')\n",
    "                                alerts_df = alerts_df.drop('date_dt', axis=1)\n",
    "                            except:\n",
    "                                date_min = alerts_df['date'].min()\n",
    "                                date_max = alerts_df['date'].max()\n",
    "                        else:\n",
    "                            date_min = alerts_df['date'].min().strftime('%Y-%m-%d')\n",
    "                            date_max = alerts_df['date'].max().strftime('%Y-%m-%d')\n",
    "                            \n",
    "                        print(f\"Date range in data: {date_min} to {date_max}\")\n",
    "                    \n",
    "                    # Count alerts by date\n",
    "                    if 'date' in alerts_df.columns:\n",
    "                        date_counts = alerts_df.groupby('date').size()\n",
    "                        print(\"\\nAlerts by date (top 10):\")\n",
    "                        for date, count in date_counts.sort_values(ascending=False).head(10).items():\n",
    "                            print(f\"  {date}: {count} alerts\")\n",
    "                    \n",
    "                    # Save the combined data\n",
    "                    print(\"\\n--- Saving Combined Data ---\")\n",
    "                    # Save with a timestamp to ensure we don't mix data\n",
    "                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                    backup_path = f\"merged_alarms_data_{timestamp}.csv\"\n",
    "                    save_to_csv(alerts_df, backup_path)\n",
    "                    print(f\"Backup saved to: {backup_path}\")\n",
    "                    \n",
    "                    # Save to main file\n",
    "                    save_to_csv(alerts_df, MERGED_ALARMS_PATH)\n",
    "                    print(f\"Combined data saved to: {MERGED_ALARMS_PATH}\")\n",
    "                else:\n",
    "                    print(\"No data was fetched from any source. Cannot continue.\")\n",
    "                    return\n",
    "            \n",
    "            # Extract unique locations and geocode them\n",
    "            if alerts_df is None or alerts_df.empty:\n",
    "                print(\"Error: No alert data available. Cannot continue.\")\n",
    "                return\n",
    "                \n",
    "            # Check if city regions file already exists\n",
    "            if os.path.exists(CITIES_REGIONS_PATH) and not force_fetch:\n",
    "                print(f\"Loading existing city region data from {CITIES_REGIONS_PATH}\")\n",
    "                city_regions_df = pd.read_excel(CITIES_REGIONS_PATH)\n",
    "            else:\n",
    "                print(\"\\n--- Extracting and Geocoding Locations ---\")\n",
    "                print(\"Extracting unique locations...\")\n",
    "                unique_locations = extract_unique_locations(alerts_df)\n",
    "                print(f\"Found {len(unique_locations)} unique locations\")\n",
    "                \n",
    "                print(\"Geocoding locations...\")\n",
    "                city_regions_df = batch_geocode_locations(unique_locations)\n",
    "                save_to_excel(city_regions_df, CITIES_REGIONS_PATH)\n",
    "                print(f\"Location data saved to: {CITIES_REGIONS_PATH}\")\n",
    "        \n",
    "        else:\n",
    "            # If starting at transform stage, load existing files\n",
    "            print(\"\\n=== Extract Stage ===\")\n",
    "            print(\"Skipping extraction, loading existing files...\")\n",
    "            try:\n",
    "                # Try multiple encodings for Hebrew text\n",
    "                try:\n",
    "                    alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8-sig')\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        alerts_df = pd.read_csv(MERGED_ALARMS_PATH, encoding='cp1255')\n",
    "                \n",
    "                city_regions_df = pd.read_excel(CITIES_REGIONS_PATH)\n",
    "                \n",
    "                print(f\"Loaded {len(alerts_df)} alerts and {len(city_regions_df)} locations\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing files: {e}\")\n",
    "                print(\"Cannot continue without data files.\")\n",
    "                return\n",
    "        \n",
    "        # Transform stage\n",
    "        if not skip_to_analysis:\n",
    "            print(\"\\n=== Transform Stage ===\")\n",
    "            \n",
    "            # Apply manual region mapping for missing locations\n",
    "            print(\"Applying manual region mappings...\")\n",
    "            updated_regions_df = apply_manual_region_mapping(city_regions_df)\n",
    "            save_to_excel(updated_regions_df, CITIES_REGIONS_UPDATED_PATH)\n",
    "            \n",
    "            # Add region information to alerts\n",
    "            print(\"Adding region information to alerts...\")\n",
    "            alerts_with_regions = add_region_to_alerts(alerts_df, updated_regions_df)\n",
    "            \n",
    "            # Translate regions to English\n",
    "            print(\"Translating regions to English...\")\n",
    "            alerts_with_english = translate_regions_to_english(alerts_with_regions)\n",
    "            \n",
    "            # Filter by central regions\n",
    "            print(\"Filtering by central regions...\")\n",
    "            central_alerts = filter_by_regions(alerts_with_english, CENTRAL_REGIONS)\n",
    "            \n",
    "            # Filter by specific dates\n",
    "            print(\"Filtering by specific dates...\")\n",
    "            filtered_alerts = filter_by_dates(central_alerts, FILTER_DATES)\n",
    "            \n",
    "            # Remove temporary columns used for filtering\n",
    "            if 'date_str' in filtered_alerts.columns:\n",
    "                filtered_alerts = filtered_alerts.drop('date_str', axis=1)\n",
    "        else:\n",
    "            print(\"\\n=== Transform Stage ===\")\n",
    "            print(\"Skipping transformation, loading existing files...\")\n",
    "            # Load necessary file for analysis\n",
    "            if os.path.exists(FINAL_FILTERED_ALARMS_PATH):\n",
    "                filtered_alerts = pd.read_excel(FINAL_FILTERED_ALARMS_PATH)\n",
    "                print(f\"Loaded {len(filtered_alerts)} filtered alerts for analysis\")\n",
    "            else:\n",
    "                print(f\"Error: {FINAL_FILTERED_ALARMS_PATH} not found. Cannot skip to analysis.\")\n",
    "                skip_to_analysis = False\n",
    "                \n",
    "                # Must perform transform stage\n",
    "                # Apply manual region mapping for missing locations\n",
    "                print(\"Applying manual region mappings...\")\n",
    "                updated_regions_df = apply_manual_region_mapping(city_regions_df)\n",
    "                save_to_excel(updated_regions_df, CITIES_REGIONS_UPDATED_PATH)\n",
    "                \n",
    "                # Add region information to alerts\n",
    "                print(\"Adding region information to alerts...\")\n",
    "                alerts_with_regions = add_region_to_alerts(alerts_df, updated_regions_df)\n",
    "                \n",
    "                # Translate regions to English\n",
    "                print(\"Translating regions to English...\")\n",
    "                alerts_with_english = translate_regions_to_english(alerts_with_regions)\n",
    "                \n",
    "                # Filter by central regions\n",
    "                print(\"Filtering by central regions...\")\n",
    "                central_alerts = filter_by_regions(alerts_with_english, CENTRAL_REGIONS)\n",
    "                \n",
    "                # Filter by specific dates\n",
    "                print(\"Filtering by specific dates...\")\n",
    "                filtered_alerts = filter_by_dates(central_alerts, FILTER_DATES)\n",
    "                \n",
    "                # Remove temporary columns used for filtering\n",
    "                if 'date_str' in filtered_alerts.columns:\n",
    "                    filtered_alerts = filtered_alerts.drop('date_str', axis=1)\n",
    "        \n",
    "        # Load stage\n",
    "        if not skip_to_analysis:\n",
    "            print(\"\\n=== Load Stage ===\")\n",
    "            \n",
    "            print(\"Saving alerts with regions...\")\n",
    "            save_to_excel(alerts_with_regions, ALARMS_WITH_REGIONS_PATH)\n",
    "            \n",
    "            print(\"Saving alerts with English regions...\")\n",
    "            save_to_excel(alerts_with_english, ALARMS_WITH_ENGLISH_REGIONS_PATH)\n",
    "            \n",
    "            print(\"Saving central region alerts...\")\n",
    "            save_to_excel(central_alerts, CENTRAL_REGIONS_ALARMS_PATH)\n",
    "            \n",
    "            print(\"Saving final filtered alerts...\")\n",
    "            save_to_excel(\n",
    "                filtered_alerts, \n",
    "                FINAL_FILTERED_ALARMS_PATH,\n",
    "                sheet_name='Filtered Alarms'\n",
    "            )\n",
    "        \n",
    "        # Analysis stage\n",
    "        print(\"\\n=== Analysis Stage ===\")\n",
    "        analyze_results(filtered_alerts)\n",
    "        \n",
    "        print(\"\\nETL Pipeline completed successfully!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in ETL pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nETL Pipeline completed with errors.\")\n",
    "\n",
    "\n",
    "# Run the pipeline if script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
